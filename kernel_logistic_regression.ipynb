{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yerbol Aussat (SID: 20698564)\n",
    "# CS698, Assignment 3\n",
    "# Kernel Logistic Regression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing and normalizing training data\n",
    "trn_data = np.genfromtxt('train_X_dog_cat.csv', delimiter=',')\n",
    "row_sums = trn_data.sum(axis=1)\n",
    "trn_data = trn_data / row_sums[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing and normalizing training labels\n",
    "trn_labels = np.genfromtxt('train_y_dog_cat.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing and normalizing test data\n",
    "test_data = np.genfromtxt('test_X_dog_cat.csv', delimiter=',')\n",
    "row_sums = test_data.sum(axis=1)\n",
    "test_data = test_data / row_sums[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing and normalizing test labels\n",
    "test_labels = np.genfromtxt('test_y_dog_cat.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(K, alpha):                                                        \n",
    "    z = np.dot(K, alpha)\n",
    "    return 1.0 / (1.0 + np.exp(-z)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradient(K, y, alpha, reg_const):   \n",
    "    # indices of positive and negative labels\n",
    "    y_pos = np.where(y == 1) # locations of y = +1\n",
    "    y_neg = np.where(y == -1) # locations of y = -1\n",
    "\n",
    "    # number of positive and negative smaples\n",
    "    n_pos = len(y_pos[0])\n",
    "    n_neg = len(y_neg[0])\n",
    "    p = sigmoid(K, alpha)\n",
    "    \n",
    "    # gradient:\n",
    "    g = 1.0/n_pos * np.dot(K[y_pos].T, (p[y_pos] - np.ones((n_pos, 1)) )) + 1.0/n_neg * np.dot(K[y_neg].T, (p[y_neg])) + 2.0 *reg_const * K.dot(alpha)\n",
    "    return g                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(K, y, alpha, reg_const):  \n",
    "    # indices of positive and negative labels\n",
    "    y_pos = np.where(y == 1) # locations of y = +1\n",
    "    y_neg = np.where(y == -1) # locations of y = -1\n",
    "    \n",
    "    # number of positive and negative smaples\n",
    "    n_pos = len(y_pos[0])\n",
    "    n_neg = len(y_neg[0])\n",
    "    p = sigmoid(K, alpha)\n",
    "    \n",
    "    # Hessian:\n",
    "    H = 1.0/n_pos * K[y_pos].T.dot(np.diagflat(p[y_pos])).dot(np.diagflat(np.ones((n_pos, 1)) - p[y_pos])).dot(K[y_pos]) + 1.0/n_neg * K[y_neg].T.dot(np.diagflat(p[y_neg])).dot(np.diagflat(np.ones((n_neg, 1)) - p[y_neg])).dot(K[y_neg]) + 2.0 * reg_const * K\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def Newton(K, y_train, reg_const):\n",
    "    max_pass = 50 # maximum number of iterations\n",
    "    alpha = np.zeros((len(K), 1)) # initializing vector alpha\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        print \"  iteration\", iteration, \": \",\n",
    "        alpha_prev = deepcopy(alpha)\n",
    "        g = gradient(K, y_train, alpha, reg_const)\n",
    "        H = hessian(K, y_train, alpha, reg_const)\n",
    "        alpha = alpha_prev - 0.5*np.linalg.solve(H, g) # updating alpha in each iteration\n",
    "        print np.linalg.norm(alpha - alpha_prev)\n",
    "        iteration += 1 \n",
    "        \n",
    "        # Stopping condition\n",
    "        if (np.linalg.norm(alpha - alpha_prev) < 10**-4) or (iteration >= max_pass):\n",
    "            return alpha           \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define three kernels\n",
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def polynomial_kernel(x, y, p=5):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=5.0):\n",
    "    return np.exp(-np.linalg.norm(x-y)**2 / sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculates percent error of kernel logistic regression algorithm\n",
    "def percent_error(kernel, trn_data, trn_labels, test_data, test_labels, lambdas, sigma=5):\n",
    "    if kernel == linear_kernel:\n",
    "        kernel_name = \"Linear Kernel\"\n",
    "    elif kernel == polynomial_kernel:\n",
    "        kernel_name = \"Polynomial Kernel\"    \n",
    "    elif kernel == gaussian_kernel:\n",
    "        kernel_name = \"Gaussian Kernel\"    \n",
    "    \n",
    "    # Calculate kernel matrix\n",
    "    print \"Calculating\", kernel_name, \"matrix\"\n",
    "    n_samples = len(trn_labels)\n",
    "    K = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            if kernel == gaussian_kernel:\n",
    "                K[i,j] = kernel(trn_data[i], trn_data[j], sigma)\n",
    "            else:\n",
    "                K[i,j] = kernel(trn_data[i], trn_data[j])\n",
    "    \n",
    "    for i in range(len(lambdas)):   \n",
    "        reg_const = lambdas[i]\n",
    "        \n",
    "        # Training kernel logistic regression algorithm for the value of lambda\n",
    "        print \"Training kernel logistic regression algorithm for lambda =\", reg_const\n",
    "        alpha_vector = Newton(K, trn_labels, reg_const)\n",
    "    \n",
    "        # Testing kernel logistic regression algorithm for lambda = 1\n",
    "        print \"Testing kernel logistic regression algorithm for lambda =\", reg_const\n",
    "        incorrect = 0\n",
    "        for sample_i in range(len(test_labels)):\n",
    "            x = test_data[sample_i]\n",
    "            K0 = np.zeros(n_samples)\n",
    "            for i in range(n_samples):  \n",
    "                if kernel == gaussian_kernel:\n",
    "                    K0[i] = kernel(trn_data[i], x, sigma)\n",
    "                else:\n",
    "                    K0[i] = kernel(trn_data[i], x)                \n",
    "            p0 = sigmoid(K0, alpha_vector) \n",
    "            if (p0 >= 0.5):\n",
    "                prediction = 1\n",
    "            else:\n",
    "                prediction = -1\n",
    "            if prediction != test_labels[sample_i]:\n",
    "                incorrect+=1\n",
    "        if kernel == gaussian_kernel:\n",
    "            print \"\\n\", kernel_name, \", lambda =\", reg_const, \", sigma =\", sigma\n",
    "        else:\n",
    "            print \"\\n\", kernel_name, \": lambda =\", reg_const, \":\"\n",
    "        print \"  Num Incorrectly Classified:\", incorrect\n",
    "        print \"  Percent Error:\", 1.0*incorrect / len(test_labels)\n",
    "        print \"\\n\", '*'*40, '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR KERNEL\n",
    "percent_error(linear_kernel, trn_data, trn_labels, test_data, test_labels, [0.1, 1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLYNOMIAL KERNEL\n",
    "percent_error(polynomial_kernel, trn_data, trn_labels, test_data, test_labels, [0.1, 1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAUSSIAN KERNEL (sigma = 5)\n",
    "percent_error(gaussian_kernel, trn_data, trn_labels, test_data, test_labels, [0.1, 1, 10], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAUSSIAN KERNEL (sigma = 10)\n",
    "percent_error(gaussian_kernel, trn_data, trn_labels, test_data, test_labels, [0.1, 1, 10], sigma=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAUSSIAN KERNEL (sigma = 1)\n",
    "percent_error(gaussian_kernel, trn_data, trn_labels, test_data, test_labels, [0.1, 1, 10], sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
